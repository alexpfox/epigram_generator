{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA / insights on poet vocabulary/complexity/pos/readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_quotes = [] # import each text file\n",
    "with open('texts/wilde_quotes.csv', 'r') as csvfile: \n",
    "    reader = csv.reader(csvfile, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        wilde_quotes.append('\\n'.join(row))\n",
    "        \n",
    "frost_quotes = [] # import each text file\n",
    "with open('texts/frost_quotes.txt', 'r') as csvfile: \n",
    "    reader = csv.reader(csvfile, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        frost_quotes.append('\\n'.join(row))\n",
    "        \n",
    "\n",
    "kaur_quotes = [] # import each text file\n",
    "with open('texts/kaur_quotes.csv', 'r') as csvfile: \n",
    "    reader = csv.reader(csvfile, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        kaur_quotes.append('\\n'.join(row))\n",
    "        \n",
    "        \n",
    "eliot_quotes = [] # import each text file\n",
    "with open('texts/eliot_quotes.txt', 'r') as csvfile: \n",
    "    reader = csv.reader(csvfile, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        eliot_quotes.append('\\n'.join(row))\n",
    "        \n",
    "        \n",
    "shakespeare_quotes = [] # import each text file\n",
    "with open('texts/shakespeare_quotes.txt', 'r') as csvfile: \n",
    "    reader = csv.reader(csvfile, delimiter='\\n')\n",
    "    for row in reader:\n",
    "        shakespeare_quotes.append('\\n'.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# import stopwords\n",
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA/ Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each poet\n",
    "\n",
    "processed_docs = []\n",
    "\n",
    "for i in np.arange(len(eliot_quotes)):\n",
    "    processed_docs.append(preprocess(eliot_quotes[i]))\n",
    "    \n",
    "# for i in np.arange(len(frost_quotes)):\n",
    "#     processed_docs.append(preprocess(frost_quotes[i]))\n",
    "    \n",
    "# for i in np.arange(len(kaur_quotes)):\n",
    "#     processed_docs.append(preprocess(kaur_quotes[i]))\n",
    "    \n",
    "# for i in np.arange(len(eliot_quotes)):\n",
    "#     processed_docs.append(preprocess(eliot_quotes[i]))\n",
    "    \n",
    "# for i in np.arange(len(shakespeare_quotes)):\n",
    "#     processed_docs.append(preprocess(shakespeare_quotes[i]))\n",
    "\n",
    "processed_docs = list(filter(None, processed_docs)) # remove empty lists  \n",
    "# print(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aunt\n",
      "1 helen\n",
      "2 maiden\n",
      "3 miss\n",
      "4 slingsbi\n",
      "5 fashion\n",
      "6 hous\n",
      "7 live\n",
      "8 near\n",
      "9 small\n",
      "10 squar\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)   \n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#bow_corpus = list(filter(None, bow_corpus)) # remove empty lists, is this useful here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.078*\"time\" + 0.067*\"hand\" + 0.064*\"think\" + 0.052*\"hair\" + 0.049*\"eye\" + 0.035*\"wind\" + 0.035*\"white\" + 0.032*\"arm\" + 0.029*\"brown\" + 0.024*\"see\"\n",
      "Topic: 1 \n",
      "Words: 0.072*\"know\" + 0.069*\"shall\" + 0.047*\"turn\" + 0.044*\"night\" + 0.039*\"give\" + 0.039*\"like\" + 0.036*\"light\" + 0.031*\"door\" + 0.026*\"sing\" + 0.026*\"look\"\n",
      "Topic: 2 \n",
      "Words: 0.054*\"water\" + 0.045*\"rock\" + 0.042*\"even\" + 0.042*\"come\" + 0.037*\"window\" + 0.028*\"break\" + 0.028*\"friend\" + 0.026*\"weav\" + 0.026*\"afternoon\" + 0.021*\"dead\"\n",
      "Topic: 3 \n",
      "Words: 0.091*\"street\" + 0.076*\"say\" + 0.040*\"feet\" + 0.039*\"leav\" + 0.034*\"room\" + 0.031*\"face\" + 0.031*\"fall\" + 0.031*\"smell\" + 0.031*\"hous\" + 0.031*\"lamp\"\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=4, id2word=dictionary, passes=75, workers=2, minimum_probability=.1)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "def get_tokens(doc):\n",
    "    # remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    for i in range(len(doc)):\n",
    "        doc[i] = doc[i].translate(translator).replace('”', '').replace('“', '') # removes smart quotes\n",
    "\n",
    "    # remove stop words and tokenize\n",
    "    texts = [[word for word in quote.lower().split() if word not in sw] for quote in doc]\n",
    "\n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 1]\n",
    "             for text in texts]\n",
    "\n",
    "    return(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilde_tokens = get_tokens(wilde_quotes)\n",
    "frost_tokens = get_tokens(frost_quotes)\n",
    "kaur_tokens = get_tokens(kaur_quotes)\n",
    "eliot_tokens = get_tokens(eliot_quotes)\n",
    "shakespeare_tokens = get_tokens(shakespeare_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "for i in np.arange(len(eliot_quotes)):\n",
    "    str(eliot_quotes[i]).map(preprocess)\n",
    "processed_docs.append(i)\n",
    "processed_docs[:10]\n",
    "\n",
    "\n",
    "processed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import operator\n",
    "\n",
    "# Create dicts with key:value as “How many times does the word <___> appear in the document? Once.”\n",
    "\n",
    "wilde_dict = corpora.Dictionary(wilde_tokens)\n",
    "wilde_dict.save('/tmp/wilde.dict')  # store the dictionary, for future reference\n",
    "# print(wilde_dict.token2id)\n",
    "# find most common word\n",
    "print(\"most frequent wilde word = \" + str(max(wilde_dict.token2id.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "frost_dict = corpora.Dictionary(frost_tokens)\n",
    "frost_dict.save('/tmp/frost.dict')\n",
    "print(\"most frequent frost word = \" + str(max(frost_dict.token2id.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "kaur_dict = corpora.Dictionary(kaur_tokens)\n",
    "kaur_dict.save('/tmp/kaur.dict') \n",
    "print(\"most frequent kaur word = \" + str(max(kaur_dict.token2id.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "eliot_dict = corpora.Dictionary(eliot_tokens)\n",
    "eliot_dict.save('/tmp/eliot.dict') \n",
    "print(\"most frequent eliot word = \" + str(max(eliot_dict.token2id.items(), key=operator.itemgetter(1))))\n",
    "\n",
    "shakespeare_dict = corpora.Dictionary(shakespeare_tokens)\n",
    "shakespeare_dict.save('/tmp/shakespeare.dict') \n",
    "print(\"most frequent shakespeare word = \" + str(max(shakespeare_dict.token2id.items(), key=operator.itemgetter(1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vader for sentiment, or textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(quotes):\n",
    "    summary = {\"positive\":0,\"neutral\":0,\"negative\":0}\n",
    "    for x in quotes: \n",
    "        ss = sid.polarity_scores(x)\n",
    "        if ss[\"compound\"] == 0.0: \n",
    "            summary[\"neutral\"] +=1\n",
    "        elif ss[\"compound\"] > 0.0:\n",
    "            summary[\"positive\"] +=1\n",
    "        else:\n",
    "            summary[\"negative\"] +=1\n",
    "    return(summary)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(wilde_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(frost_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(kaur_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(eliot_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(shakespeare_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### readability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fleisch-kincaid score, dale-chall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/spacy-readability/\n",
    "import spacy\n",
    "from spacy_readability import Readability\n",
    "\n",
    "def get_readability(quotes):\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    read = Readability()\n",
    "    nlp.add_pipe(read, last=True)\n",
    "\n",
    "    doc = nlp(quotes)\n",
    "\n",
    "#     print(doc._.flesch_kincaid_grade_level)\n",
    "#     print(doc._.flesch_kincaid_reading_ease)\n",
    "#     print(doc._.dale_chall)\n",
    "#     print(doc._.smog)\n",
    "#     print(doc._.coleman_liau_index)\n",
    "#     print(doc._.automated_readability_index)\n",
    "#     print(doc._.forcast)\n",
    "\n",
    "    stats = [doc._.flesch_kincaid_grade_level,doc._.flesch_kincaid_reading_ease,doc._.dale_chall,doc._.smog,doc._.coleman_liau_index,\\\n",
    "             doc._.automated_readability_index, doc._.forcast]\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_scoring_avg(quotes):\n",
    "\n",
    "    length = len(quotes)\n",
    "    \n",
    "    for i in np.arange(length):\n",
    "\n",
    "        fk_grade_lvl = []\n",
    "        fk_grade_ease = []\n",
    "        dc = []\n",
    "        smg = []\n",
    "        cli = []\n",
    "        a_read_indx = []\n",
    "        forecast = []\n",
    "\n",
    "        scores = get_readability(quotes[i])\n",
    "        #print(scores)\n",
    "        \n",
    "        fk_grade_lvl.append(scores[0])\n",
    "        fk_grade_ease.append(scores[1])\n",
    "        dc.append(scores[2])\n",
    "        smg.append(scores[3])\n",
    "        cli.append(scores[4])\n",
    "        a_read_indx.append(scores[5])\n",
    "        forecast.append(scores[6])\n",
    "\n",
    "        avg_fk_grade_lvl = 0\n",
    "        avg_fk_grade_ease = 0\n",
    "        avg_dc = 0\n",
    "        avg_smg = 0\n",
    "        avg_cli = 0\n",
    "        avg_a_read_indx = 0\n",
    "        avg_forecast = 0\n",
    "        \n",
    "        max_fk_grade_lvl = 0\n",
    "        max_fk_grade_ease = 0\n",
    "        max_dc = 0\n",
    "        max_smg = 0\n",
    "        max_cli = 0\n",
    "        max_a_read_indx = 0\n",
    "        max_forecast = 0\n",
    "        \n",
    "        for n in fk_grade_lvl:\n",
    "            avg_fk_grade_lvl += n\n",
    "            avg_fk_grade_lvl = avg_fk_grade_lvl/length\n",
    "            \n",
    "            if n > max_fk_grade_lvl:\n",
    "                max_fk_grade_lvl = n\n",
    "            \n",
    "        for n in fk_grade_ease:\n",
    "            avg_fk_grade_ease += n\n",
    "            avg_fk_grade_ease = avg_fk_grade_ease/length\n",
    "            \n",
    "            if n > max_fk_grade_ease:\n",
    "                max_fk_grade_ease = n\n",
    "            \n",
    "        for n in dc:\n",
    "            avg_dc += n\n",
    "            avg_dc = avg_dc/length\n",
    "            \n",
    "            if n > max_dc:\n",
    "                max_dc = n            \n",
    "            \n",
    "        for n in smg:\n",
    "            avg_smg += n\n",
    "            avg_smg = avg_smg/length\n",
    "            \n",
    "            if n > max_smg:\n",
    "                max_smg = n            \n",
    "                        \n",
    "        for n in cli:\n",
    "            avg_cli += n\n",
    "            avg_cli = avg_cli/length\n",
    "            \n",
    "            if n > max_cli:\n",
    "                max_cli = n       \n",
    "            \n",
    "            \n",
    "        for n in a_read_indx:\n",
    "            avg_a_read_indx += n\n",
    "            avg_a_read_indx = avg_a_read_indx/length\n",
    "            \n",
    "            if n > max_a_read_indx:\n",
    "                max_a_read_indx = n\n",
    "            \n",
    "            \n",
    "        for n in forecast:\n",
    "            avg_forecast += n\n",
    "            avg_forecast = avg_forecast/length \n",
    "            \n",
    "            if n > max_forecast:\n",
    "                max_forecast = n\n",
    "        \n",
    "    return [\"flesch_kincaid_grade_level = \" + str(avg_fk_grade_lvl), \"flesch_kincaid_reading_ease = \"+ str(avg_fk_grade_ease), \\\n",
    "            \"dale_chall = \" + str(avg_dc), \"smog = \" + str(avg_smg), \"coleman_liau_index = \" + str(avg_cli), \\\n",
    "            \"automated_readability_index = \" + str(avg_a_read_indx), \"forcast = \" + str(avg_forecast), \\\n",
    "            \"max flesch_kincaid_grade_level = \" + str(max_fk_grade_lvl), \"max flesch_kincaid_reading_ease = \"+ str(max_fk_grade_ease), \\\n",
    "            \"max dale_chall = \" + str(max_dc), \"max smog = \" + str(max_smg), \"max coleman_liau_index = \" + str(max_cli), \\\n",
    "            \"max automated_readability_index = \" + str(max_a_read_indx), \"max forcast = \" + str(max_forecast)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_scoring_avg(wilde_quotes)\n",
    "#get_scoring_avg(frost_quotes)\n",
    "#get_scoring_avg(kaur_quotes)\n",
    "#get_scoring_avg(eliot_quotes)\n",
    "get_scoring_avg(shakespeare_quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with https://pypi.org/project/textstat/ instead\n",
    "# import textstat\n",
    "\n",
    "# test_data = wilde_quotes[1]\n",
    "\n",
    "# print(textstat.flesch_reading_ease(test_data))\n",
    "# print(textstat.smog_index(test_data))\n",
    "# print(textstat.flesch_kincaid_grade(test_data))\n",
    "# print(textstat.coleman_liau_index(test_data))\n",
    "# print(textstat.automated_readability_index(test_data))\n",
    "# print(textstat.dale_chall_readability_score(test_data))\n",
    "# print(textstat.difficult_words(test_data))\n",
    "# print(textstat.linsear_write_formula(test_data))\n",
    "# print(textstat.gunning_fog(test_data))\n",
    "# print(textstat.text_standard(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "\n",
    "text = wilde_quotes[0]\n",
    "ss = nt.sent_tokenize(text)\n",
    "tokenized_sent = [nt.word_tokenize(sent) for sent in ss]\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(quotes):\n",
    "    \n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract parts of speech, store in df, export to csv, add to visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllable count comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_syllable_superlatives(quote_list):\n",
    "    count = 0 # using to avoid calling syllable_count twice\n",
    "    max_syllables = 0\n",
    "    total_syllables = 0\n",
    "    words=0\n",
    "    ignore = ['Gwendolen–Cecily–it', 'Societycivilized', \\\n",
    "              'Squarethecircletillyouretired', 'somebodydirector', 'blackpapercovered',\\\n",
    "              'roofsPolyphiloprogenitive']\n",
    "         \n",
    "    for quote in quote_list:\n",
    "        for word in quote.split():\n",
    "            words+=1 # track num words to get avg later\n",
    "            count = syllable_count(word)\n",
    "            \n",
    "            if count > max_syllables:\n",
    "                if word in ignore: # if a known bogus word, ignore it in our average count\n",
    "                    words-=1 \n",
    "                else: # word must be longer and real\n",
    "                    max_syllables = count\n",
    "                    \n",
    "                if word not in ignore:\n",
    "                    print(word) # sanity check to make sure we don't include concatted words\n",
    "            total_syllables +=count    \n",
    "                \n",
    "    return \"max = \" + str(max_syllables), \"avg = \" + str(total_syllables/words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From\n",
      "fairest\n",
      "creatures\n",
      "self-substantial\n",
      "determination:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('max = 5', 'avg = 1.2968848242354958')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_syllable_superlatives(wilde_quotes)\n",
    "#get_syllable_superlatives(frost_quotes)\n",
    "#get_syllable_superlatives(kaur_quotes)\n",
    "#get_syllable_superlatives(eliot_quotes)\n",
    "get_syllable_superlatives(shakespeare_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllable_count(\"superlative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf \n",
    "\n",
    "wilde_corpus = []\n",
    "for line in wilde_tokens:\n",
    "    wilde_corpus.append(wilde_dict.doc2bow(line))\n",
    "\n",
    "frost_corpus = []\n",
    "for line in frost_tokens:\n",
    "    frost_corpus.append(frost_dict.doc2bow(line))\n",
    "      \n",
    "kaur_corpus = []\n",
    "for line in kaur_tokens:\n",
    "    kaur_corpus.append(kaur_dict.doc2bow(line))\n",
    "\n",
    "eliot_corpus = []\n",
    "for line in eliot_tokens:\n",
    "    eliot_corpus.append(eliot_dict.doc2bow(line))    \n",
    "\n",
    "shakespeare_corpus = []\n",
    "for line in shakespeare_tokens:\n",
    "    shakespeare_corpus.append(shakespeare_dict.doc2bow(line))    \n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "wilde_tfidf= TfidfModel(wilde_corpus)\n",
    "\n",
    "frost_tfidf= TfidfModel(frost_corpus)\n",
    "\n",
    "kaur_tfidf= TfidfModel(kaur_corpus)\n",
    "\n",
    "eliot_tfidf= TfidfModel(eliot_corpus)\n",
    "\n",
    "shakespeare_tfidf= TfidfModel(shakespeare_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
